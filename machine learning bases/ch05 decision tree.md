### 决策树
#### 决策树学习的三个步骤
1. 特征选择
2. 决策树的生成
3. 决策树的修剪
---
#### 决策树模型
决策树由节点和有向边组成，节点有两种类型：内部节点和叶节点。内部节点表示一个特征或属性，叶节点表示一个类。
用决策树分类方法：
从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子节点，这时每个子节点对应着该特征的一个取值，如此递归地对实例进行测试并分配，直至达到叶节点。
#### 决策树与条件概率分布
决策树表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分上，将特征空间划分为互不相交的单元或区域，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。
假设X为表示特征的随机变量，Y为表示类的随机变量，那么这个条件概率分布可以表示为$P(Y|X)$

---
#### 特征选择
**特征选择的目的**
特征选择在于选取对训练数据具有分类能力的特征。
**特征选择的准则**
信息增益/信息增益比
**信息增益**
* 熵
定义：熵表示随机变量不确定性的度量。
    * 设X一个取有限个值的离散随机变量，其概率分布为：
$$P(X=x_i) = p_i, i = 1, 2, ... , n
$$
则**随机变量X**的熵定义为
$$H(X) = -\sum_{i=1}^{n}p_i\log{p_i}$$
通常，上式中的对数以2为底或以e为底。由上面的定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可以将X的熵记为H(p)：
$$H(P) = -\sum_{i=1}^{n}p_i\log{p_i}$$
熵越大，随机变量的不确定性越大。从定义可验证
$$0 \leq H(p) \leq \log{n}$$
  * 设有随机变量$(X, Y)$，其联合概率分布为：
    $$
    P(X=x_i, Y = y_j) = p_{ij}, i = 1, 2, ... , n,j = 1, 2, ... , m
    $$
    条件熵$H(Y|X)$表示在已知随机变量X的条件下，随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵$H(Y|X)$，定义为**X给定条件下Y的条件概率分布的熵对X的数学期望**。
    $$
    H(Y|X) = \sum_{i=1}^{n}p_iH(Y|X=x_i), p_i = P(X = x_i), i = 1, 2, ..., n
    $$
    当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。
* 信息增益
  特征A对训练数据集D的信息增益$g(D, A)$， 定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即
  $$g(D, A) = H(D) - H(D|A)$$
  * 信息增益的理解：得知特征X的信息而使得类Y的信息的不确定性减少的程度。
  * 根据信息增益选择特征：
    给定训练数据集D和特征A，经验熵$H(D)$表示对数据集D进行分类的不确定性，而经验条件熵$H(D|A)$表示在特征A给定的条件下对数据集D进行分类的不确定性。那么他们的差，就是信息增益，表示由于特征A而使得对数据集D进行分类的不确定性减少的程度。
    对于数据集D而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。
  * 熵与条件熵之差称为互信息，决策树学习中的信息增益等价于训练数据集中雷雨特征的互信息
  * 特征A带来的信息增益越大，表示特征A确定后类别的不确定性减少的更多，对于当前数据集特征A就更优
##### 算法：信息增益算法
设训练数据集D，｜D｜为训练数据集的样本容量，即样本个数。
设有k个类，$C_k$，k = 1, 2, 3,...,K $|C_k|$为属于类，$C_k$，的样本个数，$\sum_{k=1}^{N}|C_K| = |D|$
设特征A有n个不同的取值$\displaystyle\left\{a_1, a_2, ..., a_n\right\}$,根据特征A的取值将D划分为n个不同的子集$D_1, D_2,...,D_n$, $|D_i|$为子集$D_i$的样本个数，$\sum_{i=1}^{N}|D_i| = |D|$
记子集$D_i$中属于类$C_k$的样本的集合为$D_{ik}$,记$D_{ik} = D_i\cap C_k$
则信息增益算法如下：
（1）计算数据集D的经验熵$H(D)$
$$
H(D) = -\sum_{k=1}^K\displaystyle\frac{|C_k|}{|D|}\log_{2}{\displaystyle\frac{|C_k|}{|D|}}
$$
（2）计算特征A对数据集D的条件经验熵$H(D|A)$
$$
H(D|A) = \sum_{i=1}^{n}\displaystyle\frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^{n}\displaystyle\frac{|D_{i}|}{|D|}\sum_{k=1}^{K}\displaystyle\frac{|D_{ik}|}{|D_i|}\log_{2}{\displaystyle\frac{|D_{ik}|}{|D_i|}}
$$
（3）计算信息增益
$$
g(D, A) = H(D) - H(D|A)
$$
* 信息增益比
使用信息增益存在的问题：以信息增益作为划分训练数据集的特征，存在**偏向于选择取值较多的特征**的问题，使用信息增益比可以对这个问题进行校正。
定义：特征A对训练数据集D的信息增益比$g_R(D, A)$， 定义为其信息增益$g(D, A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比
$$
g_R(D, A) = \displaystyle\frac{g(D, A)}{H_A(D)}
$$
其中，$H_A(D） = -\sum_{i=1}^{n}\displaystyle\frac{|D_i|}{|D}\log_{2}{\displaystyle\frac{|D_i|}{|D|}}$， n是特征A取值的个数
---
#### 决策树的生成
##### ID3算法
核心：在决策树各个节点上应用**信息增益**准则选择特征，递归地构建决策树。
不足：ID3算法只有树的生成，所以该算法生成的树容易过拟合。
##### C4.5算法
核心：在决策树各个节点上应用**信息增益比**选择特征，递归地构建决策树。

---
#### 决策树的剪枝pruning
决策树的剪枝往往通过极小化决策树整体的损失函数(oss function)或代价函数(lost function)来实现。
**决策树学习的损失函数**
设树T的叶节点个数为|T|，t是T的叶节点，该叶节点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，k = 1,2,3,...,K。
$H_t(T)$为叶节点t上的经验熵，$\alpha \leq 0$为参数，则决策树学习的损失函数定义如下: 决策树所有叶节点的经验熵之和加上一个$\alpha|T|$
$$
C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T) + \alpha|T|
$$
其中经验熵为
$$
H_t(T) = -\sum_{k}\displaystyle\frac{N_{tk}}{N_t}\log_{2}{\displaystyle\frac{N_{tk}}{N_t}}
$$
在损失函数中，将右侧第一项记为$C(T)$，则损失函数可以表示为
$C_{\alpha}(T) = C(T) + \alpha|T|$
其中，$C(T)$表示模型对训练数据的预测误差即模型与训练数据的拟合程度，｜T｜表示模型的复杂度。
##### 算法：树的剪枝算法
1. 计算每个节点的经验熵
2. 递归地从树的叶节点向上回缩。如果剪枝前的整体经验熵大于剪枝后的，则进行剪枝操作。
