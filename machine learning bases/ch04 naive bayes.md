# 第四章 朴素贝叶斯法
---
基本原理：基于贝叶斯定理与特征条件独立假设
注意：朴素贝叶斯和贝叶斯估计是不同的概念
基本步骤：
1. 基于特征条件独立假设学习输入输出的联合概率分布
2. 基于联合概率分布，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y

---
### 一些概率的基础知识
先验分布：
根据一般的经验认为随机变量应该满足的分布。如：根据往年的气候经验（经验），推测下雨（结果）的概率即为先验概率
后验分布：
通过当前训练数据修正的随机变量的分布，比先验分布更符合当前的数据。如：有乌云（原因、观测数据）的时候，下雨（结果）的概率即为后验概率
似然估计：
已知训练数据，给定了模型，通过让似然性极大化估计模型参数的一种方法。如：下雨（结果）的时候有乌云（观测数据，原因等）的概率即为似然概率
>后验分布往往是基于先验分布和极大似然估计计算出来的

#### 贝叶斯公式（后验概率公式、逆概率公式）

$p(\theta|x) = \displaystyle\frac{p(x|\theta)p{(\theta)}}{p(x)}$
$P(A_i|A) = \displaystyle\frac{P(A_i)P(A|A_i)}{\sum_{j=1}^NP(A_j)P(A|A_j)}$
推导过程：https://mathworld.wolfram.com/BayesTheorem.html
$\theta$：决定数据分布的参数（原因）
x: 观察得到的数据（结果）

p(x):证据因子evidence

$p(\theta)$:先验概率

$p(\theta|x)$:
后验概率：参数已知，样本是变量
后验概率和条件概率的关系：
对于一个随机变量来说，量化其不确定性非常重要。其中一个实现方法便是提供其后验概率的置信区间……
后验概率是条件概率在某一框定情境下的更细化概念


$p(x|\theta)$:似然概率：样本已知，参数是变量
后验概率 = 似然函数*先验概率/证据因子
联合概率：P(AB)=P(A)P(B|A)=P(B)P(A|B)


--- 
### 朴素贝叶斯的基本方法
1. 根据特征条件独立假设学习联合概率分布
朴素贝叶斯法通过训练数据集学习联合概率分布：
联合分布 = 先验概率 * 条件概率
* 先验概率：$P(Y = c_k), k = 1,2,3...$
* 条件概率分布：（1）式
$$
  P(X = x|Y = c_k) = P(X^{(1)} = x^{(1)},...,X^{(n)} = x^{(n)}| Y = c_k), k = 1,2,3...
$$
存在的问题：条件概率分布（1）式有指数级数量的参数，其估计实际是不可行的
解决方法：朴素贝叶斯法对条件概率分布作了条件独立性的假设，这是一个较强的假设
条件独立性假设 
含义：用于分类的特征在类确定的条件下都是条件独立的；这一假设使朴素贝叶斯变得简单，但是有时会牺牲一定的准确率
**(2)式**
$$
\begin{aligned}
P(X = x | Y = c_k) \\
&= P(X^{(1)} = x^{(1)},...,X^{(n)} = x^{(n)}| Y = c_k)\\
&=\prod_{j=1}^nP(X^{(j)}= x^{(j)} | Y = c_k)
\end{aligned}
$$
2. 分类时：根据学到的联合概率分布，计算后验概率分布
原理：朴素贝叶斯法将样本分到后验概率最大的类中，后验概率最大等价于0-1损失函数时的期望风险最小化
对给定的输入x，通过学习到的模型计算后验概率分布$$P(Y = c_k | X = x)$$将后验概率最大的类作为x的输出
后验概率计算根据贝叶斯定理进行 **(3)式**
$$
\begin{aligned}
P(Y = c_k | X = x) = \displaystyle\frac{P(X = x | Y = c_k)P(Y = c_k)}{\sum_{k}P(X = x | Y = c_k)P(Y = c_k)}
\end{aligned}
$$
将（2）带入（3），得到朴素贝叶斯分类的基本公式 **(4)式**
$$
y = f(x) = argmax_{ck} = \displaystyle\frac{P(Y = c_k)\prod_jP(X^{(j)} = x^{(j)}|Y = c_k)}{\sum_kP(Y = c_k)\prod_jP(X^{(j)} = x^{(j)}|Y = c_k)}
$$
因为4式分母对所有$c_k$都是相同的，所以简化如下
$$
y = argmax_{ck}P(Y = c_k)\prod_jP(X^{(j)} = x^{(j)} | Y = c_k)
$$
含义是：让后面这个概率最大的$c_k$的值
---
### 朴素贝叶斯法的参数估计
在朴素贝叶斯法中，学习意味着估计$P(Y = c_k)$和$P(X^{(j)} = x^{(j)} | Y = c_k)$
对这两个估计，可以使用极大似然估计/贝叶斯估计（两种最常用的参数估计方法）
1. 极大似然估计：是典型的频率学派观点
2. 贝叶斯估计：是典型的贝叶斯学派观点
#### 极大似然估计
基本思想：待估计参数$\theta$是客观存在的，只是未知而已，当$\theta_{mle}$满足$\theta=\theta_{mle}$，若该组观测样本更容易被观测到，就可以说$\theta_{mle}$是$\theta$的极大似然估计值。也可以理解为：估计值$\theta_{mle}$使事件出现的可能性最大
$L(\theta|x) = f(x|\theta) = \prod_{i=1}^nf(x_i|\theta)$
$\theta_{mle} = argmax_{theta}L(\theta|x)$

#### 贝叶斯估计
基本思想：待估计参数$\theta$也是随机的，和一般随机变量没有本质区别，因此只能根据观测样本估计参数$\theta$的分布
贝叶斯估计的数学描述：贝叶斯估计利用了贝叶斯公式
$P(\theta|x) = \displaystyle\frac{P(\theta)P(x|\theta)}{\sum_{j=1}^NP(\theta_j)P(x|\theta_j)}$
其中$P(\theta)$是参数$\theta$的先验分布，表示对参数$\theta$的主观认识，是非样本信息。$P(\theta|x)$是参数$\theta$的后验分布，因此贝叶斯估计可以看做，在假定$\theta$服从$P(\theta)$的先验分布前提下，根据样本信息去校正先验分布，得到后验分布$P(\theta|x)$.
因为后验分布是一个条件分布，通常取后验分布的期望作为参数的估计值
$\theta_{be} = EP(\theta|x)$

在朴素贝叶斯法中，用极大似然估计可能会出现所要估计的概率值为0的情况，使分类结果产生偏差。那么多概率连乘，如果其中有一个概率为0怎么办？那整个式子直接就是0了，这样不对。所以我们连乘中的每一项都得想办法让它保证不是0，哪怕它原先是0,（如果原先是0，表示在所有连乘项中它概率最小，那么转换完以后只要仍然保证它的值最小，对于结果的大小来说没有影响。）。解决这个问题就需要使用贝叶斯估计。也就是在极大似然估计的基础上分子分母各加上一个正数。当正数$\lambda$=1时，称为拉普拉斯平滑（Laplacian smoothing）
#### 最大后验估计MAP
如果在贝叶斯估计中，考虑极大似然估计的思想，将后验分布极大化而去求解$\theta$，就变成了最大后验估计MAP（在贝叶斯公式中求后验概率最大值的那个参数）
$$
\theta_{map} = argmax_{\theta}P(\theta)P(x|\theta) = argmax_{\theta}(\log{P(x|\theta)} + \log{P(\theta)})
$$
作为贝叶斯估计的一种近似解，MAP有其存在的价值，因为贝叶斯估计中后验分布的计算往往是非常棘手的；而且，MAP并非简单地回到极大似然估计，它依然利用了来自先验的信息，这些信息无法从观测样本获得。
如果将机器学习结构风险中的正则化项对应为上式的$\log{\theta}$，那么带有正则化项的最大似然学习就可以被解释为MAP。当然，这并不是总是正确的，例如，有些正则化项可能不是一个概率分布的对数，还有些正则化项依赖于数据，当然也不会是一个先验概率分布。不过，MAP提供了一个直观的方法来设计复杂但可解释的正则化项，例如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一个单独的高斯分布。

---
### 朴素贝叶斯算法
* 计算先验概率及条件概率
* 对于给定的实例，计算后验概率
* 确定实例x的类

### 代码实现

### 课后习题